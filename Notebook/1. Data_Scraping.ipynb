{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "### Since for this task we need good amout of data. I had tried to scrap it using Beautiful soup but have some problem as we are geting less instances and some of the information was unretrievable. So I used Reddit API for this, which gives a flexible api to retrive data based on limits.\n",
    "I do used Beautiful soup for data extraion in Web app. (For demo purpose)\n",
    "***\n",
    "## Reference\n",
    "\n",
    "- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Reddit API](https://praw.readthedocs.io/en/latest/getting_started/configuration/prawini.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have used these major Flairs for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs_list = [\"AskIndia\", \n",
    "          \"Non-Political\", \n",
    "          \"Policy/Economy\",\n",
    "          \"Politics\", \n",
    "          \"Photography\",\n",
    "          \"Business/Finance\",\n",
    "          \"Science/Technology\",\n",
    "          \"Sports\", \n",
    "          \"Scheduled\",\n",
    "          \"Food\", \n",
    "          \"Coronavirus\",\n",
    "          \"AMA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have created this dictionary so that it will be easily converted in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flair - List of all Post Flair\n",
    "# Title - List of all Post Title\n",
    "# score - List of all scores of respective post\n",
    "# full_link - List of all Post Url addresss\n",
    "# selftext - List of all Post Body\n",
    "# Author - List of all Post Author\n",
    "# combined_comments - List of top 5 cascaded comments\n",
    "\n",
    "flair_dictionary = { \"Flair\":[],\n",
    "                     \"Title\":[],\n",
    "                     \"Created_at\":[],\n",
    "                     \"Score\":[],\n",
    "                     \"Url_address\":[],\n",
    "                     \"Body\":[],\n",
    "                     \"Author\":[],\n",
    "                     \"num_comments\":[],\n",
    "                     \"combined_comments\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing `Reddit API`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Library Reddit Library\n",
    "import praw\n",
    "# Initializing Reddit API\n",
    "reddit = praw.Reddit(client_id='KhELuATyIFR1aQ',\n",
    "                     client_secret='gHm5vxbsctlwSE8yqtpLKAdFR7E',\n",
    "                     user_agent='ahmadkhan',\n",
    "                        username='ahmadkhan242', \n",
    "                     password='9990800652')\n",
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting submission respective above chosen flairs.\n",
    "<h4>Here we are extracting same no of post for each flair we defined in the above cell. Here we use search option given by reddit API to search most relevent data for give flair.<h4/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AskIndia:  78%|███████▊  | 234/300 [2:05:44<35:27, 32.24s/it]    \n",
      "Non-Political:  72%|███████▏  | 216/300 [05:35<02:10,  1.55s/it]\n",
      "Policy/Economy:  73%|███████▎  | 220/300 [06:13<02:15,  1.70s/it]\n",
      "Politics:  82%|████████▏ | 246/300 [17:23<03:48,  4.24s/it]  \n",
      "Photography:  74%|███████▍  | 222/300 [05:25<01:54,  1.47s/it]\n",
      "Business/Finance:  78%|███████▊  | 233/300 [08:20<02:23,  2.15s/it] \n",
      "Science/Technology:  74%|███████▎  | 221/300 [19:44<07:03,  5.36s/it]   \n",
      "Sports:  77%|███████▋  | 231/300 [07:42<02:18,  2.00s/it]\n",
      "Scheduled:  78%|███████▊  | 234/300 [09:03<02:33,  2.32s/it] \n",
      "Food:  81%|████████  | 242/300 [19:44<04:43,  4.89s/it]  \n",
      "Coronavirus:  83%|████████▎ | 249/300 [1:52:27<23:02, 27.10s/it]    \n",
      "AMA:  71%|███████   | 212/300 [34:38<14:22,  9.80s/it]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Choosing India as subrediit\n",
    "subreddit = reddit.subreddit('india')\n",
    "# Iterating over flairs_list\n",
    "\n",
    "for flair in flairs_list:\n",
    "    collect_subreddit = subreddit.search (flair, limit=None)\n",
    "    with tqdm(total=300, desc=flair) as pbar:\n",
    "        for i in collect_subreddit:\n",
    "            flair_dictionary[\"Flair\"].append (flair)\n",
    "            flair_dictionary[\"Title\"].append (i.title)\n",
    "            flair_dictionary[\"Created_at\"].append (i.created)\n",
    "            flair_dictionary[\"Score\"].append (i.score)\n",
    "            flair_dictionary[\"Url_address\"].append(i.url)\n",
    "            flair_dictionary[\"Body\"].append(i.selftext)\n",
    "            flair_dictionary[\"Author\"].append (i.author)\n",
    "            flair_dictionary[\"num_comments\"].append (i.num_comments)\n",
    "            i.comments.replace_more(limit=None)\n",
    "            combined_comments = \" \"\n",
    "            c = 0\n",
    "            try:\n",
    "                for comment in i.comments:\n",
    "                    combined_comments += \" \" + comment.body\n",
    "                    c+=1\n",
    "                    if c>11 :\n",
    "                        break\n",
    "            except:\n",
    "                combined_comments = combined_comments\n",
    "            flair_dictionary[\"combined_comments\"].append(combined_comments)\n",
    "            pbar.update(1)\n",
    "        if(flair_dictionary['Flair'].count(flair) >= 300):\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using Pandas we convert `flair_dictionary` in dataframe and save the DataFrame as CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Creating DataFrame from flairs_dictionary \n",
    "my_data = pd.DataFrame(flair_dictionary)\n",
    "\n",
    "# Saveing above DataFrame as .csv file on Local\n",
    "my_data.to_csv('flair_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why to choose similar number of post for each Flair?\n",
    "Post on Reddit are not evenly distribueted based on flairs. Post appears in random basis sometime they change with time like if we take flairs for this year till now, Coronavirus flair appears more than photograpy. So if we take data randomly it is possible that we get more data for one flair than another, this will lead the model to overfit on one flair than all of them. So for unbaise classification we take same number of post data for each Flair.  \n",
    "```Analysis of data based on date of Submission is in EDA section to validate this claim.```  \n",
    "[Refrence 1](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/)  \n",
    "[Refrence 2](https://elitedatascience.com/imbalanced-classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Data extraction using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title =  Surveillance drone in my red-zone\n",
      "Flair =  Coronavirus\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://www.reddit.com/r/india/comments/fxo4xy/surveillance_drone_in_my_redzone/'\n",
    "headers ={\"User-Agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\"}\n",
    "with requests.Session() as s:\n",
    "    r = s.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    Titles = soup.find_all('h1', class_=\"_eYtD2XCVieq6emjKBH3m\")\n",
    "    Flair_div = soup.find_all('div', class_=\"_2X6EB3ZhEeXCh1eIVA64XM\")\n",
    "    Flair = Flair_div[0].select_one(\"span\")\n",
    "    print(\"Title = \", Titles[0].text)\n",
    "    print(\"Flair = \", Flair.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow GPU)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}